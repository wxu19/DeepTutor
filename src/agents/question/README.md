# Question Generation System

Intelligent question generation system with **dual-mode architecture** supporting both knowledge base-driven custom generation and reference exam paper mimicking.

## ğŸ“ Directory Structure

```
question/
â”œâ”€â”€ __init__.py                    # Module exports
â”œâ”€â”€ agents/                        # Agent classes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py             # Base agent class (ReAct paradigm)
â”‚   â”œâ”€â”€ generation_agent.py       # Question generation agent
â”‚   â””â”€â”€ validation_agent.py       # Question validation agent (deprecated)
â”œâ”€â”€ tools/                         # Tools and utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_parser.py             # PDF parsing with MinerU
â”‚   â”œâ”€â”€ question_extractor.py     # Extract questions from exams
â”‚   â””â”€â”€ exam_mimic.py             # Reference-based question generation
â”œâ”€â”€ prompts/                       # Bilingual prompts (YAML)
â”‚   â”œâ”€â”€ zh/                       # Chinese prompts
â”‚   â”‚   â”œâ”€â”€ generation_agent.yaml
â”‚   â”‚   â”œâ”€â”€ validation_workflow.yaml
â”‚   â”‚   â””â”€â”€ coordinator.yaml
â”‚   â””â”€â”€ en/                       # English prompts
â”‚       â””â”€â”€ (same structure)
â”œâ”€â”€ coordinator.py                # Agent coordinator
â”œâ”€â”€ validation_workflow.py        # Validation workflow
â”œâ”€â”€ example.py                    # Usage examples
â””â”€â”€ README.md
```

## ğŸš€ Core Features

### Custom Mode (Knowledge Base-Driven)
- âœ… **Intelligent Search Query Generation** - Generates configurable number of RAG queries (from `config/main.yaml`)
- âœ… **Background Knowledge Acquisition** - Retrieves relevant knowledge using naive mode RAG
- âœ… **Question Planning** - Creates comprehensive plan with question IDs, focuses, and types
- âœ… **Single-Pass Validation** - Analyzes question relevance without iterative refinement
- âœ… **Complete File Persistence** - Saves background knowledge, plan, and individual results

### Mimic Mode (Reference Exam-Based)
- âœ… **PDF Parsing** - Automatic PDF extraction using MinerU
- âœ… **Question Extraction** - Identifies and extracts reference questions from exams
- âœ… **Style Mimicking** - Generates similar questions based on reference structure
- âœ… **Batch Organization** - All outputs saved to timestamped folders
- âœ… **Progress Tracking** - Real-time updates for parsing, extracting, and generating stages

### Common Features
- âœ… **Multimodal Content Support** - Correctly parses text, equations, images, and tables
- âœ… **Batch Question Generation** - Handles multiple questions in parallel
- âœ… **Bilingual Prompts** - Supports both Chinese and English prompts via YAML configuration
- âœ… **Real-time WebSocket Streaming** - Live progress updates to frontend

## System Architecture

### Custom Mode Architecture
```
User Requirement
    â†“
Generate RAG Queries (configurable count)
    â†“
Retrieve Background Knowledge (naive mode)
    â†“
Create Question Plan (IDs, focuses, types)
    â†“
For each question:
    GenerationAgent â†’ ValidationWorkflow
    â†“                     â†“
  [retrieve]         [analyze_relevance]
  [generate]         (single-pass, no rejection)
    â†“
Save: background_knowledge.json
      question_plan.json
      question_X_result.json
```

### Mimic Mode Architecture
```
PDF Upload / Parsed Directory
    â†“
Parse PDF with MinerU
    â†“
Extract Reference Questions
    â†“
For each reference question (parallel):
    GenerationAgent â†’ ValidationWorkflow
    â†“
Save to: mimic_YYYYMMDD_HHMMSS_{pdf_name}/
    â”œâ”€â”€ {pdf_name}.pdf
    â”œâ”€â”€ auto/{pdf_name}.md
    â”œâ”€â”€ {pdf_name}_questions.json
    â””â”€â”€ {pdf_name}_generated_questions.json
```

## Core Components

### 1. BaseAgent (Base Agent Class)

Located in `agents/base_agent.py`. Implements ReAct loop: **Think â†’ Act â†’ Observe**

### 2. QuestionGenerationAgent

Located in `agents/generation_agent.py`. Generates questions based on knowledge base content or reference questions.

**Available Actions** (Custom Mode):
- `retrieve`: Retrieve relevant knowledge from knowledge base
- `generate_question`: Generate questions based on retrieved knowledge
- `refine_question`: Modify questions based on validation feedback
- `submit_question`: Submit question to validation workflow

**Key Changes**:
- âŒ Removed `reject_task` action - no longer rejects tasks
- âœ… Focuses on generation and refinement only

### 3. QuestionValidationWorkflow

Located in `validation_workflow.py`. Validates question quality using single-pass analysis.

**Custom Mode Workflow**:
- `retrieve` â†’ `validate` â†’ `analyze_relevance` â†’ `return`

**Validation Output**:
- `decision`: "approve" (always, no rejection)
- `relevance`: "highly_relevant" | "partially_relevant"
- `kb_coverage`: Detailed analysis of knowledge base content tested
- `extension_points`: Explanation of extensions beyond KB (if applicable)
- `issues`: List of quality issues (if any)
- `suggestions`: Improvement suggestions (for refinement)

**Mimic Mode Workflow**:
- Similar structure, but validates against reference question style and knowledge base sufficiency

### 4. AgentCoordinator

Located in `coordinator.py`. Manages question generation workflows for both custom and mimic modes.

**Custom Mode Methods**:
- `generate_questions_custom()`: Full pipeline from requirement text to final questions
- `_generate_search_queries_from_text()`: Creates RAG queries (count from config)
- `_create_question_plan()`: Generates question plan based on requirements
- `generate_question()`: Single question generation with validation

**Mimic Mode**:
- Handled by `tools/exam_mimic.py` with coordinator for individual question generation

## Usage

### Custom Mode - Full Pipeline

```python
import asyncio
from src.agents.question import AgentCoordinator

async def main():
    # Create coordinator
    coordinator = AgentCoordinator(
        max_rounds=10,
        kb_name="math2211",
        output_dir="data/user/question"
    )

    # Full pipeline from text requirement
    result = await coordinator.generate_questions_custom(
        requirement_text="Generate 3 medium-difficulty questions about multivariable limits",
        difficulty="medium",
        question_type="choice",
        count=3
    )

    print(f"âœ… Generated {result['completed']}/{result['requested']} questions")
    for q in result['results']:
        print(f"- {q['question']['question'][:50]}...")

asyncio.run(main())
```

### Custom Mode - Single Question

```python
# Define question requirement
requirement = {
    "knowledge_point": "Limits and continuity of multivariable functions",
    "difficulty": "medium",
    "question_type": "choice",
    "focus": "Test path-dependent limits at (0,0)"
}

# Generate single question
result = await coordinator.generate_question(requirement)

if result["success"]:
    print(f"âœ… Generated in {result['rounds']} rounds")
    print(f"Question: {result['question']['question']}")
    print(f"Relevance: {result['validation']['relevance']}")
else:
    print(f"âŒ Failed: {result['error']}")
```

### Mimic Mode - PDF Upload

```python
from src.agents.question.tools.exam_mimic import mimic_exam_questions

result = await mimic_exam_questions(
    pdf_path="exams/midterm.pdf",
    kb_name="math2211",
    output_dir="data/user/question/mimic_papers",
    max_questions=5
)

print(f"âœ… Generated {result['successful_generations']} questions")
print(f"Output: {result['output_file']}")
```

### Mimic Mode - Parsed Directory

```python
# If you already have MinerU parsed results
result = await mimic_exam_questions(
    paper_dir="data/parsed_exams/exam_20240101",
    kb_name="math2211",
    max_questions=None  # Generate for all reference questions
)
```

## Prompts Configuration

Prompts are stored in YAML files under `prompts/` directory with bilingual support:

- `prompts/en/` - English prompts
- `prompts/zh/` - Chinese prompts

Each agent loads prompts based on the `language` parameter (default: "en").

### YAML Format Example

```yaml
system: |
  You are a professional Question Generation Agent...

generate: |
  Generate a question based on the following information:
  Requirements: {requirements}
  Retrieved knowledge: {knowledge}
  ...

refine: |
  Please modify the question based on validation feedback:
  ...
```

## Configuration

### Environment Variables

```bash
# LLM API Configuration
LLM_API_KEY=your_api_key_here
LLM_HOST=https://api.openai.com/v1
LLM_MODEL=gpt-4o
```

### Configuration Files

**`config/main.yaml`** - RAG query count:
```yaml
question:
  rag_query_count: 3  # Number of RAG queries for background knowledge
```

**`config/agents.yaml`** - Agent parameters:
```yaml
question:
  temperature: 0.7
  max_tokens: 4000
```

## Tools

### PDF Parsing (`tools/pdf_parser.py`)

Uses MinerU for high-quality PDF extraction with formula and table support.

```python
from src.agents.question.tools import parse_pdf_with_mineru

# Parse PDF to markdown
success = await parse_pdf_with_mineru(
    pdf_path="/path/to/exam.pdf",
    output_base_dir="data/user/question/mimic_papers"
)
```

### Question Extraction (`tools/question_extractor.py`)

Extracts structured questions from parsed exam papers.

```python
from src.agents.question.tools import extract_questions_from_paper

questions = await extract_questions_from_paper(
    paper_dir="data/mimic_papers/exam_20241211",
    output_dir="data/mimic_papers"
)
# Returns: [{"question_number": "1", "question_text": "...", "images": []}, ...]
```

### Exam Mimicking (`tools/exam_mimic.py`)

Complete pipeline for reference-based question generation.

```python
from src.agents.question.tools import mimic_exam_questions

result = await mimic_exam_questions(
    pdf_path="exams/midterm.pdf",  # Or use paper_dir for parsed
    kb_name="math2211",
    output_dir="data/user/question/mimic_papers",
    max_questions=5,
    ws_callback=None  # Optional: async callback for progress updates
)
```

## Return Format

### Custom Mode - Single Question

```python
{
    "success": True,
    "question": {
        "question_type": "choice",
        "question": "Question content",
        "options": {"A": "...", "B": "...", "C": "...", "D": "..."},
        "correct_answer": "A",
        "explanation": "Detailed explanation",
        "knowledge_point": "Topic name"
    },
    "validation": {
        "decision": "approve",  # Always approve, no rejection
        "relevance": "highly_relevant",  # or "partially_relevant"
        "kb_coverage": "This question tests...",  # For highly relevant
        "extension_points": "This question extends...",  # For partially relevant
        "issues": [],  # Quality issues if any
        "suggestions": []  # Improvement suggestions
    },
    "rounds": 1  # Number of generation rounds (typically 1-2)
}
```

### Custom Mode - Batch Generation

```python
{
    "completed": 3,
    "requested": 3,
    "failed": 0,
    "results": [
        {
            "question": {...},
            "validation": {...},
            "rounds": 1
        },
        ...
    ]
}
```

### Mimic Mode - Output

```python
{
    "reference_paper": "exam_name",
    "kb_name": "math2211",
    "total_reference_questions": 5,
    "successful_generations": 5,
    "failed_generations": 0,
    "generated_questions": [
        {
            "success": True,
            "reference_question_number": "1",
            "reference_question_text": "Original question...",
            "generated_question": {...},
            "validation": {...},
            "rounds": 2
        },
        ...
    ],
    "output_file": "data/user/question/mimic_papers/mimic_20241211_120000_exam/..."
}
```

## Output Files

### Custom Mode Directory Structure

```
data/user/question/custom_YYYYMMDD_HHMMSS/
â”œâ”€â”€ background_knowledge.json       # RAG retrieval results
â”‚   {
â”‚     "queries": ["query1", "query2", "query3"],
â”‚     "knowledge": {
â”‚       "query1": {"chunks": [...], "entities": [...], "relations": [...]},
â”‚       ...
â”‚     }
â”‚   }
â”‚
â”œâ”€â”€ question_plan.json              # Question planning
â”‚   {
â”‚     "focuses": [
â”‚       {"id": "q_1", "focus": "...", "type": "choice"},
â”‚       {"id": "q_2", "focus": "...", "type": "written"},
â”‚       ...
â”‚     ]
â”‚   }
â”‚
â”œâ”€â”€ question_1_result.json          # Individual question + validation
â”œâ”€â”€ question_2_result.json
â””â”€â”€ ...
```

### Mimic Mode Directory Structure

```
data/user/question/mimic_papers/
â””â”€â”€ mimic_YYYYMMDD_HHMMSS_{pdf_name}/
    â”œâ”€â”€ {pdf_name}.pdf                                      # Original PDF
    â”œâ”€â”€ auto/{pdf_name}.md                                  # MinerU parsed markdown
    â”œâ”€â”€ {pdf_name}_YYYYMMDD_HHMMSS_questions.json          # Extracted reference questions
    â”‚   {
    â”‚     "total_questions": 4,
    â”‚     "questions": [
    â”‚       {"question_number": "1", "question_text": "...", "images": []},
    â”‚       ...
    â”‚     ]
    â”‚   }
    â”‚
    â””â”€â”€ {pdf_name}_YYYYMMDD_HHMMSS_generated_questions.json # Generated questions
        {
          "reference_paper": "...",
          "kb_name": "...",
          "total_reference_questions": 4,
          "successful_generations": 4,
          "generated_questions": [
            {
              "reference_question_text": "...",
              "generated_question": {...},
              "validation": {...}
            },
            ...
          ]
        }
```

## Key Changes from Previous Version

### âœ… What's New

1. **Dual-Mode Architecture**
   - Custom mode: Knowledge base-driven generation
   - Mimic mode: Reference exam paper mimicking

2. **Simplified Validation**
   - Single-pass validation (no iterative refinement)
   - Always approves questions (no rejection)
   - Analyzes relevance: `highly_relevant` vs `partially_relevant`

3. **Enhanced File Persistence**
   - All intermediate files saved (background knowledge, plan)
   - Mimic mode: Timestamped batch folders
   - Complete traceability for debugging

4. **Configurable RAG Queries**
   - `rag_query_count` in `config/main.yaml`
   - Previously hardcoded to 3

5. **Real-time Progress Tracking**
   - WebSocket streaming for live updates
   - Mimic mode stages: uploading â†’ parsing â†’ extracting â†’ generating

### âŒ What's Removed

1. **Task Rejection Logic**
   - Removed `reject_task` action from generation agent
   - Questions always proceed to validation
   - Validation analyzes relevance instead of rejecting

2. **Iterative Refinement**
   - No multi-round validation loop
   - Single-pass generation + validation
   - Reduces LLM calls and cost

3. **Standalone Validation Agent**
   - Merged into `ValidationWorkflow`
   - Simplified architecture

## Migration Notes

If upgrading from the old structure:

**File Structure Changes:**
- `base_agent.py` â†’ `agents/base_agent.py`
- `question_generation_agent.py` â†’ `agents/generation_agent.py`
- `question_validation_agent.py` â†’ `agents/validation_agent.py` (deprecated)
- `question_tools/retrieve.py` â†’ Removed (use RAG tool directly)
- `parse_pdf_with_mineru.py` â†’ `tools/pdf_parser.py`
- `extract_questions_from_paper.py` â†’ `tools/question_extractor.py`
- `mimic_exam_paper.py` â†’ `tools/exam_mimic.py`
- `question_validation_workflow.py` â†’ `validation_workflow.py`

**Import Path Updates:**
```python
# Old
from .base_agent import BaseAgent
from .question_tools import retrieve

# New
from .agents import BaseAgent
from src.tools import rag_search  # Use RAG tool directly
```

**API Changes:**
```python
# Old - with rejection handling
result = await coordinator.generate_question(requirement)
if result.get("error") == "task_rejected":
    print("Agent rejected task")

# New - always generates, check relevance
result = await coordinator.generate_question(requirement)
if result["success"]:
    relevance = result["validation"]["relevance"]
    if relevance == "highly_relevant":
        print(result["validation"]["kb_coverage"])
    else:
        print(result["validation"]["extension_points"])
```
